{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By open source Python package "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mdptoolbox** in Python provides classes and functions for the resolution of descrete-time Markov Decision Processes. The algorithms provided includes backwards induction, linear programming, policy iteration, q-learning and value iteration.\n",
    "\n",
    "The **quantecon** python library consists of a number of modules among which is the one for solving dynamic programs (Markov decision processes) with finite states and actions.\n",
    "\n",
    "First install quantecon by opening a terminal prompt and typing: ** pip install quantecon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import quantecon as qe   # cmd: pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider the following example, taken from Puterman (2005), Section 3.1, pp.33-35.\n",
    "<img src=\"MDP example.png\">\n",
    "\n",
    "    Set of states S = {0, 1}\n",
    "\n",
    "    Set of actions A = {0, 1}\n",
    "\n",
    "    Set of feasible state-action pairs SA = {(0, 0), (0, 1), (1, 0)}\n",
    "\n",
    "    Rewards r(s, a):\n",
    "\n",
    "        r(0, 0) = 5, r(0, 1) =10, r(1, 0) = -1\n",
    "\n",
    "    Transition probabilities q(s_next|s, a):\n",
    "\n",
    "    Action0    q(0|0, 0) = 0.5, q(1|0, 0) = 0.5,  q(0|1, 0) = 0,  q(1|1, 0) = 1,\n",
    "    Action1    q(0|0, 1) = 0, q(1|0, 1) = 1.\n",
    "\n",
    "    Discount factor = 0.95\n",
    "\n",
    "Creating a `DiscreteDP` instance.\n",
    "\n",
    "This approach uses the product set S x A as the domain and treat action 1 as yielding a reward negative infinity at state 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R1 = [[5, 10], [-1, -float('inf')]]   # product set  S x A \n",
    "Q = [[(0.5, 0.5), (0, 1)], [(0, 1), (0.5, 0.5)]]\n",
    "beta = 0.95                                  # Discount factor\n",
    "ddp = qe.markov.ddp.DiscreteDP(R1, Q, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimal policy function =  [0 0]\n",
      " Optimal value function =  [ -8.57137463 -19.99994606]\n",
      " Number of iterations = 250\n"
     ]
    }
   ],
   "source": [
    "res = ddp.solve(method='value_iteration', v_init=[0, 0],epsilon=10**(-5))\n",
    "print( \" Optimal policy function = \", res.sigma)   \n",
    "print(\" Optimal value function = \",res.v)     \n",
    "print(\" Number of iterations =\", res.num_iter)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming  - value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimal policy function =  [ 0.  0.]\n",
      " Optimal value function =  [ -8.57124383 -19.99981526]\n",
      " Number of iterations = 226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_state = 2 # Number of STATES\n",
    "num_action = 2  # Number of ACTIONS\n",
    "tol = 10**(-5) # Maximum tolerance between two iterations of the value function until we say that the value function converged\n",
    "\n",
    "R = [[5, -1], [10, -float('inf')]]     #Income after each action;  product set A x S\n",
    "value_old = np.ones(num_state) # Initialize the value function guess (current iteration)\n",
    "value_new = np.zeros(num_state) # Initialize the value function guess (next iteration)\n",
    "policy = np.zeros(num_state) # Initialize the policy function guess\n",
    "ctr = 0 # Counter for the iterations until convergence\n",
    "\n",
    "while(np.max(np.abs(value_old-value_new)) > tol): # Computing the max difference between value function iterations\n",
    "    np.copyto(value_old,value_new) # Replace value_old by value_new\n",
    "    for i in np.arange(num_state): # Update the value function for each possible\n",
    "        aa = np.ones(num_action)\n",
    "        for j in np.arange(num_action):\n",
    "            aa[j]=R[j][i]+(Q[j][i]@value_old)*beta\n",
    "        value_new[i] = np.max(aa)\n",
    "        policy[i] = np.argmax(aa)  # Find the ACTION to maximize value\n",
    "    ctr = ctr+1 # Increment counter\n",
    "    \n",
    "    \n",
    "print( \" Optimal policy function = \", policy)   # Optimal policy function\n",
    "print(\" Optimal value function = \",value_new)       # Optimal value function\n",
    "print(\" Number of iterations =\", ctr)   # Number of iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
